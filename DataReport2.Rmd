---
title: "BIOL3207 Data Report 2"
author: "u6956268"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Load required packages
```{r}
library(pacman)
p_load(bookdown, tidyverse, flextable, metafor, orchaRd)
```


#### Link to GitHub Repository
[My GitHub Repository](https://github.com/nicolelehmann/BIOL3207_Report2)


#### Analysis of OA_activitydat_20190302_BIOL3207.csv to generate summary statistics (mean, SD, N) for each fish species average activity for each treatment group
```{r}
data <- read_csv("OA_activitydat_20190302_BIOL3207.csv")
data_clean <- data[which(complete.cases(data)),] # Remove data points which are incomplete
data_clean <- select(data_clean, -...1) # Remove irrelevant column

summary_stats <- data_clean %>% group_by(species, treatment) %>% summarise(n(), mean(activity), sd(activity))
summary_stats <- pivot_wider(data=summary_stats, names_from="treatment", values_from=c("n()", "mean(activity)", "sd(activity)"))
summary_stats
```

#### Merge these summary statistics with the meta-data in clark_paper_data.csv
```{r}
clark_data <- read_csv("clark_paper_data.csv")
clark_data <- clark_data %>% slice(rep(1:n(), each = 6)) #Copy row of info 6 times according to formatting of meta-analysis data set

clark_data <- mutate(clark_data, Species=summary_stats$species, .after=10)
clark_data[which(clark_data$Species=="acantho"),]$Species <- "Acanthochromis polyacanthus"
clark_data[which(clark_data$Species=="ambon"),]$Species <- "Pomacentrus amboinensis"
clark_data[which(clark_data$Species=="chromis"),]$Species <- "Chromis atripectoralis"
clark_data[which(clark_data$Species=="humbug"),]$Species <- "Dascyllus aruanus"
clark_data[which(clark_data$Species=="lemon"),]$Species <- "Pomacentrus moluccensis"
clark_data[which(clark_data$Species=="whitedams"),]$Species <- "Dischistodus perspicillatus"

clark_data <- mutate(clark_data, 
                    ctrl.n=summary_stats$`n()_control`, 
                    ctrl.mean=summary_stats$`mean(activity)_control`, 
                    ctrl.sd=summary_stats$`sd(activity)_control`, 
                    oa.n=summary_stats$`n()_CO2`, 
                    oa.mean=summary_stats$`mean(activity)_CO2`, 
                    oa.sd=summary_stats$`sd(activity)_CO2`)
clark_data
```

#### Merge this combined summary statistics/meta-data with the larger meta-analysis data set in ocean_meta_data.csv
```{r}
meta_data <- read_csv("ocean_meta_data.csv")
clark_data$`Pub year IF` <- as.character(clark_data$`Pub year IF`) #Changing variable class to character to match meta-analysis data set 
clark_data$`2017 IF` <- as.character(clark_data$`2017 IF`) #Changing variable class to character to match meta-analysis data set 

meta_data <- full_join(meta_data, clark_data)
summary(meta_data)
```

```{r}
ggplot(meta_data, aes(y=oa.mean))+
  geom_boxplot()
```

From the data summary and plot there are four clear outlier data points with very high means and standard deviations for both the control and treatment conditions. This will dramatically skew the data if left in, so these four points will be investigated. 

```{r}
meta_data %>% filter(meta_data$oa.mean > 2000)
```

It seems like the seemingly outlier data could be due to all four rows investigating the larvae life stage, could be related to the species each study looked at or could be a measurement error in both of the studies. Let's first investigate all of the larvae data.

```{r}
meta_data %>% filter(meta_data$`Life stage`=="Larvae")
```

There are plenty of other data points for the Larvae life stage which seem to fit with the rest of the data, so this doesn't seem to be the reason for the high numbers. Let's investigate the fish species. 

```{r}
meta_data %>% filter(meta_data$Species=="Atherina presbyter"|meta_data$Species=="Clupea harengus")
```

Again there is other data for both species which fits with the rest of the data. It seems like these two studies have extremely high values possibly due to a measurement difference or a unit error. It is clear that there has been an error in the standardisation of values for the meta-analysis. As such, it seems reasonable to exclude both of these studies from the meta-data. 

```{r}
meta_data <- meta_data %>% filter(meta_data$Title!="Effects of ocean acidification on the swimming ability, development and biochemical responses of sand smelt larvae" & meta_data$Title!="Growth performance and survival of larval Atlantic herring, under the combined effects of elevated temperatures and CO2")
```


#### Calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s escalc() function
```{r}
corr_stats <- cor.test(meta_data$ctrl.mean, meta_data$oa.mean, method="pearson")
meta_data$r_corr <- corr_stats$estimate
meta_data <- metafor::escalc(measure = "ROM", 
                                n1i = ctrl.n, n2i = oa.n,
                                m1i = ctrl.mean, m2i = oa.mean, 
                                sd1i = ctrl.sd, sd2i = oa.sd, 
                                ri = r_corr,
                                var.names=c("lnRR","V_lnRR"),
                                data = meta_data)
meta_data <- meta_data %>% mutate(residual = 1:n())
meta_data$V_lnRR <- abs(meta_data$V_lnRR)
```

```{r}
ggplot(meta_data, aes(x=V_lnRR))+
  geom_histogram(binwidth = 1000000)
```
There are clearly some data points with extremely high variance compared to the vast majority of the studies. These will be excluded. More explanation... There are also some extremely small variances. 

Filter out high and low variances
```{r}
meta_data <- meta_data %>% filter(meta_data$V_lnRR<50) # Need reasoning!! 
meta_data <- meta_data %>% filter(meta_data$V_lnRR>0.0001)
```


#### Meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor’s rma.mv() function.
```{r}
model <- metafor::rma.mv(lnRR~1, V=V_lnRR, 
                         method="REML",
                         random=list(~1|Study,
                                     ~1|Behavioural.metric,
                                     ~1|residual),
                         dfs = "contain",
                         test="t",
                         data=meta_data)
model
```

```{r}
orchaRd::i2_ml(model, data = meta_data)
```


#### Written paragraph about the findings and what they mean. Support with a figure. Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals). Measures of heterogeneity in effect size estimates across studies (i.e., I2 and/or prediction intervals - see predict() function in metafor). Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure

```{r}
ggplot(meta_data, aes(x=ctrl.mean, y=oa.mean))+
  geom_point()+
  geom_smooth(method = "lm")+
  labs(x="Means of control groups", y="Means of treatment groups")
```

#### Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure.

K (787) is the number of effect sizes, the brackets give the number of studies (90). The size of the effect is scaled by the precision of each effect size value. 
```{r}
orchaRd::orchard_plot(model, group = "Study", data = meta_data, xlab = "Log response ratio (lnRR)", angle = 45)
```


Overall meta-analytic mean is -0.1845. This means that for every 1 increase in the control mean, the treatment mean increases by 0.8155 (1-0.1845). 
95% confidence interval is -0.4066 to 0.0375. This confidence interval includes 0 which is the null hypothesis so there is no statistical significance. It is very possible that there is no difference between the control and treatment conditions. 
There is a significant amount of heterogeneity among effects, with Q=736,088,491, df=798, p=<0.0001. Proportion of variance among effects after removing sampling variation is 100%. Differences between studies explains 12% of the effect size variation, and differences between Behavioural metric explains 88% of the effect size variation. 

#### Generate a funnel plot. Visually assess the possibility of publication bias.

```{r}
metafor::funnel(x = meta_data$lnRR, vi = meta_data$V_lnRR, 
                yaxis = "seinv", digits = 2, 
                level = c(0.1, 0.05, 0.01), 
                shade = c("white", "gray55", "gray 75"), las = 1, 
                xlab = "Log response ratio (lnRR)", atransf=tanh, legend = TRUE)
```


#### Generate a time-lag plot. Assess how effect sizes may or may not have changed through time.
```{r}
ggplot(meta_data, aes(y = lnRR, x = Year..online., size = 1/sqrt(V_lnRR))) + 
  geom_point(alpha = 0.30) + 
  geom_smooth(method = lm, col = "red", show.legend = FALSE) + 
  labs(x = "Online Publication Year", y = "Log Response Ratio (lnRR)", size = "Precision (1/SE)") + 
  theme_classic()
```


#### Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias.
```{r}
metafor::rma(yi = es, vi = Ves, method = "FE", data = dataFE)

```

#### Formal meta-regression model that includes inverse sampling variance (1/variance of log response ratio) to test for file-drawer biases
```{r}

```

#### Written paragraph about the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

The log-response ratio is definitely converging on the value of 0 over time. In the first couple of years, there is a strong bias towards studies with negative log response ratios. Then in the middle there is a bias towards studies with positive log response ratios. By 2020, the studies are finding log response ratios close to 0. Also note that 

#### Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et al? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

