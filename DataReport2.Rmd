---
title: "BIOL3207 Data Report 2"
author: "u6956268"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    number_sections: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Here is my [GitHub Repository](https://github.com/nicolelehmann/BIOL3207_Report2)

  
#### Load required packages
```{r loadpacks, message=FALSE, results='hide'}
library(pacman)
p_load(bookdown, tidyverse, flextable, metafor, orchaRd)
```

  
#### Summary statistics for Clark et al. data
```{r message=FALSE}
data <- read_csv("OA_activitydat_20190302_BIOL3207.csv")
data_clean <- data[which(complete.cases(data)),] # Remove data points which are incomplete
data_clean <- select(data_clean, -...1) # Remove irrelevant column which just doubles the row number

summary_stats <- data_clean %>% group_by(species, treatment) %>% summarise(n(), mean(activity), sd(activity))
summary_stats <- pivot_wider(data=summary_stats, names_from="treatment", values_from=c("n()", "mean(activity)", "sd(activity)"))
summary_stats
```

  
#### Merge summary statistics with the rest of the paper meta-data
```{r}
clark_data <- read_csv("clark_paper_data.csv")
clark_data <- clark_data %>% slice(rep(1:n(), each = 6)) #Copy row of info 6 times according to format of meta-analysis data set

clark_data <- mutate(clark_data, Species=summary_stats$species, .after=10)
clark_data[which(clark_data$Species=="acantho"),]$Species <- "Acanthochromis polyacanthus"
clark_data[which(clark_data$Species=="ambon"),]$Species <- "Pomacentrus amboinensis"
clark_data[which(clark_data$Species=="chromis"),]$Species <- "Chromis atripectoralis"
clark_data[which(clark_data$Species=="humbug"),]$Species <- "Dascyllus aruanus"
clark_data[which(clark_data$Species=="lemon"),]$Species <- "Pomacentrus moluccensis"
clark_data[which(clark_data$Species=="whitedams"),]$Species <- "Dischistodus perspicillatus"

clark_data <- mutate(clark_data, 
                    ctrl.n=summary_stats$`n()_control`, 
                    ctrl.mean=summary_stats$`mean(activity)_control`, 
                    ctrl.sd=summary_stats$`sd(activity)_control`, 
                    oa.n=summary_stats$`n()_CO2`, 
                    oa.mean=summary_stats$`mean(activity)_CO2`, 
                    oa.sd=summary_stats$`sd(activity)_CO2`)
flextable(clark_data)
```


#### Merge Clark et al. data with the meta-analysis data set
```{r}
meta_data <- read_csv("ocean_meta_data.csv")
clark_data$`Pub year IF` <- as.character(clark_data$`Pub year IF`) #Changing variable class to character to match meta-analysis data set 
clark_data$`2017 IF` <- as.character(clark_data$`2017 IF`) #Changing variable class to character to match meta-analysis data set 

meta_data <- full_join(meta_data, clark_data)
summary(meta_data)
```

```{r treatmentmeansd, fig.align='center', fig.cap="Means and corresponding standard deviations for each treatment condition group"}
ggplot(meta_data, aes(x=oa.mean, y=oa.sd))+
  geom_point()+
  labs(x="Mean for treatment groups", y="Standard deviation for treatment groups")
```

From the data summary and Fig. \@ref(fig:treatmentmeansd) there are four clear outlier data points with very high means and standard deviations. This will dramatically skew the data means and variance if left in, so these four points will be investigated further. 

```{r}
outliers <- meta_data %>% filter(meta_data$oa.mean > 2000)
flextable(outliers)
```

It seems like the outlier data points could be due to all four rows investigating the larvae life stage, could be related to the species each study looked at or could be a measurement/unit error in both of the studies. Let's first investigate all of the larvae data.

```{r}
larv <- meta_data %>% filter(meta_data$`Life stage`=="Larvae")
summary(larv)
```

There are plenty of other data points for the Larvae life stage which seem to fit with the rest of the data, so this doesn't seem to be the reason for the high numbers. However, the influence of these outlier points on the data is made obvious by the dramatic difference between mean and median values for many of the variables. Let's investigate the fish species for each of the four points. 

```{r}
AP_CH <- meta_data %>% filter(meta_data$Species=="Atherina presbyter"|meta_data$Species=="Clupea harengus")
summary(AP_CH)
```

Again there is other data for both species which fits with the rest of the data. 
Therefore, it seems like these two studies have extremely high values possibly due to a measurement difference or a unit error. Either way, it is clear that there has been an error in the standardisation of values for the meta-analysis. These are the only data points for each of the studies. As such, it seems reasonable to exclude both of these studies entirely from the meta-data. 

```{r}
meta_data <- meta_data %>% filter(meta_data$Title!="Effects of ocean acidification on the swimming ability, development and biochemical responses of sand smelt larvae" & meta_data$Title!="Growth performance and survival of larval Atlantic herring, under the combined effects of elevated temperatures and CO2")
```


#### Calculate the log response ratio (lnRR) effect sizes
```{r calclnRR, warning=FALSE}
meta_data <- metafor::escalc(measure = "ROM", 
                                n1i = ctrl.n, n2i = oa.n,
                                m1i = ctrl.mean, m2i = oa.mean, 
                                sd1i = ctrl.sd, sd2i = oa.sd, 
                                var.names=c("lnRR","V_lnRR"),
                                data = meta_data)
meta_data <- meta_data %>% mutate(residual = 1:n())
```

```{r}
highvar <- meta_data %>% filter(meta_data$V_lnRR>50)
flextable(highvar)
lowvar <- meta_data %>% filter(meta_data$V_lnRR<=0.0001)
flextable(lowvar)
```
There are clearly some data points with an extremely high log response ratio variance compared to the vast majority of the data. These data points have a log response ratio magnitudes higher than most of the data. These will be excluded because data with such a high variance must be severely under-powered and therefore unreliable. There are also some extremely small variances which will also be excluded. Even for really high-powered studies, the variance would be expected to be greater than 0.0001. Looking at the studies, they all have relatively small sample sizes so one would definitely expect biological variance to contribute to a greater variance of the log response ratio. These extremely small variances are unrealistic so the studies will be assumed to either have errors or be a fluke and will be excluded. 

Filter out high and low variances
```{r}
meta_data <- meta_data %>% filter(meta_data$V_lnRR<50)
meta_data <- meta_data %>% filter(meta_data$V_lnRR>0.0001)
```


#### Meta-analytic multivariate linear model with random effects of study and observation
```{r}
model <- metafor::rma.mv(lnRR~1, V=V_lnRR, 
                         method="REML",
                         random=list(~1|Study,
                                     ~1|Life.stage),
                         dfs = "contain",
                         test="t",
                         data=meta_data)
model
```


#### Multivariate model heterogeneity tests
```{r}
orchaRd::i2_ml(model, data = meta_data)
```

```{r}
predict(model)
```


The overall meta-analytic mean is 0.0396. This means that for every 1 increase in the control mean, the treatment mean increases by 1.0396. The 95% confidence interval is -0.0902 to 0.1694. This confidence interval includes 0 which is predicted under the null hypothesis, so there is no statistically significant difference between the treatment and control groups. The null hypothesis is retained that ocean acidification does not have any effect on behaviour. 

As seen in Fig. \@ref(fig:conditionmeanmodel), although there is some variability between studies there is a general linear trend between the means of the control and treatment groups. For example, if the mean of the control group is 500, then the mean of the treatment group is also likely to be around 500. This shows that there is no statistical difference between the control and treatment groups, so there is no indication that ocean acidification alters behaviours. 

There is heterogeneity among effects, indicated by Q=73,608.95, df=786, p=<0.0001 (highly significant). The proportion of variance among effects after removing sampling variation is 96.5%. An I^2 above 75% indicates a high level of heterogeneity. Only about 3.5% of variation seen across effects is driven by sampling variance. Differences between studies explains 96.5% of the effect size variation, and differences between Life stage explains 0.000017% of the effect size variation. The prediction intervals of -1.1452 to 1.2244 give the range of effect size values that would be expected 95% of the time from re-sampling the population. This gives a measure for how variable the effects are likely to be, and an interval of around -1 to 1 is a relatively narrow interval so there should be relatively little variation in the effect size obtained if the population was re-sampled in additional experiments.

```{r conditionmeanmodel, message=FALSE, fig.align='center', fig.cap="Linear relationship between the mean of control and treatment groups in each study"}
ggplot(meta_data, aes(x=ctrl.mean, y=oa.mean))+
  geom_point()+
  geom_smooth(method = "lm")+
  labs(x="Means of control groups", y="Means of treatment groups")
```

```{r}
lifestage_model <- metafor::rma.mv(lnRR~Life.stage, V=V_lnRR, 
                         method="REML",
                         random=list(~1|Study),
                         dfs = "contain",
                         test="t",
                         data=meta_data)
lifestage_model
```

This is a multivariate model accounting for Life stage as a potential variable contributing to the effect sizes obtained in each study. However, as seen in the estimates, the adult life stage effect size is very close to 0, and although the juvenile, larvae and not provided life stages all have a slightly smaller effect size than the adult, none of these are even close to being significantly different as they are all very close to 0 as well. It doesn't appear that life stage is affecting the effect size. 

#### Forest plot

```{r forestplot, fig.align='center', fig.cap="Forest plot showing the range of log response ratios with the point size indicating the precision of each effect size value"}
orchaRd::orchard_plot(lifestage_model, mod="Life.stage", group = "Study", data = meta_data, xlab = "Log response ratio (lnRR)", angle = 45)
```

In Fig. \@ref(fig:forestplot), K indicates the number of effect sizes in the group and the brackets indicate the number of studies in the group. The size of the effect is scaled by the precision of each effect size value. It is clear that most of the studies are sitting around a log response ratio of 0, leading to mean estimates around 0 which indicates no difference between the control and treatment groups. Ocean acidification does not seem to have an effect on behaviour. Additionally, although the larvae and juvenile life stages have greater variation in log response ratio than the adult or not provided life stages, there does not seem to be any particular difference in log response ratio between studies according to the life stage of the fish. 


#### Funnel plot

```{r funnelplot, fig.align='center', fig.cap="Funnel plot of log response ratio and precision"}
metafor::funnel(x = meta_data$lnRR, vi = meta_data$V_lnRR, 
                yaxis = "seinv", digits = 2, 
                shade = c("white", "gray 75"), las = 1, 
                xlab = "Log response ratio (lnRR)", ylab = "Precision (inverse standard error)",
                legend = TRUE)
```

From the funnel plot in Fig. \@ref(fig:funnelplot), it seems that there is some publication bias towards negative log response ratios. There is a gap on the bottom right hand side of the funnel plot where low precision studies (potentially due to being under-powered) would have found a positive log response ratio. This does not mean that positive log response ratios were not found in studies, but indicates that these studies for whatever reason did not get published, whereas the studies with negative log response ratios were highly published even when they were low precision. 

#### Time-lag plot
```{r timelag, fig.align='center', fig.cap="Time-lag plot of the log response ratio and online publication year with the point size indicating the level of precision"}
ggplot(meta_data, aes(y = lnRR, x = Year..online., size = 1/sqrt(V_lnRR))) + 
  geom_point(alpha = 0.30) + 
  geom_smooth(method = lm, col = "red", show.legend = FALSE) + 
  labs(x = "Online Publication Year", y = "Log Response Ratio (lnRR)", size = "Precision (1/SE)") + 
  theme_classic()
```

#### Formal meta-regression model with year as a moderator

```{r}
meta_data <- meta_data  %>% mutate(Year_c = Year..online. - mean(Year..online.))
#metafor::rma(yi = lnRR, vi = V_lnRR, method = "FE", data = meta_data)
metafor::rma.mv(lnRR ~ Year_c, V = V_lnRR,
                   method="REML",
                   random=list(~1|residual), 
                   dfs = "contain",
                   test="t",
                   data=meta_data)
```

There is a highly significant time-lag bias in this meta-data. As seen in the time-lag plot Fig. \@ref(fig:timelag), over time the effect sizes converge on 0. In the meta-regression model, the effect size is very close 0, so the null hypothesis is retained. However, the online publication year which is the moderator is highly significant, indicating the time-lag bias visible in the plot. 

#### Formal meta-regression model that includes inverse sampling variance (1/variance of log response ratio) to test for file-drawer biases
```{r}
# Including sampling variance as moderator

metareg_time <- rma.mv(lnRR ~ Year_c, V = 1/V_lnRR, 
                    random = list(~1|Study, 
                                  ~1|residual), 
                    test = "t", dfs = "contain", 
                    data = meta_data)
summary(metareg_time)

# How much variation does time when results were published explain in lnRR?
r2_time <- orchaRd::r2_ml(metareg_time) 
r2_time
```

These statistically significant results indicate file-drawer biases. The year that results were published can explain a large amount of the variation in effect sizes, where earlier publications found larger effect sizes. The earlier publications found majority negative log response ratios. This shows that there is likely a file-drawer problem, as authors probably did not publish the positive log response ratio results due to opposing previous study results. One would want to have a high amount of precision before publishing findings that oppose previous studies, especially if they were published in high profile journals.

#### Written paragraph about the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

The log-response ratio is definitely converging on the value of 0 over time. In the first couple of years, there is a strong bias towards studies with negative log response ratios. Then in the middle there is a bias towards studies with positive log response ratios which drags the trend towards 0. By 2020, the studies are finding log response ratios close to 0. Also note that the early studies that found more extreme log response ratios typically have lower precision, whereas the higher precision studies are closer to a log response ratio of 0. 

#### Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et al? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

Any studies contributing to publication bias?? 

The study that stands out in the funnel plot above as having a relatively high negative log response ratio and relatively high precision compared to other studies is likely contributing to publication bias. With high precision and a significant effect size, this study likely had high impact when it was published. However, the meta-analysis has found that this is a false finding. 

A meta-analysis by Clements et al. of the impacts of ocean acidification on fishe behaviour looked at 91 studies. They concluded that the large effects described in initial studies have disappeared in subsequent studies. This is the same conclusion that was found in this meta-analysis. 
Clements et al. explained the results through findings that the initial studies typically had small sample sizes but were published in high-impact journals and therefore had a large influence on the field. 

