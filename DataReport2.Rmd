---
title: "BIOL3207 Data Report 2"
author: "u6956268"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Load required packages
```{r}
library(pacman)
p_load(bookdown, tidyverse, flextable, metafor, orchaRd)
```


#### Link to GitHub Repository
Here is my [GitHub Repository](https://github.com/nicolelehmann/BIOL3207_Report2)


#### Analysis of OA_activitydat_20190302_BIOL3207.csv to generate summary statistics (mean, SD, N) for each fish species average activity for each treatment group
```{r}
data <- read_csv("OA_activitydat_20190302_BIOL3207.csv")
data_clean <- data[which(complete.cases(data)),] # Remove data points which are incomplete
data_clean <- select(data_clean, -...1) # Remove irrelevant column

summary_stats <- data_clean %>% group_by(species, treatment) %>% summarise(n(), mean(activity), sd(activity))
summary_stats <- pivot_wider(data=summary_stats, names_from="treatment", values_from=c("n()", "mean(activity)", "sd(activity)"))
summary_stats
```

#### Merge these summary statistics with the meta-data in clark_paper_data.csv
```{r}
clark_data <- read_csv("clark_paper_data.csv")
clark_data <- clark_data %>% slice(rep(1:n(), each = 6)) #Copy row of info 6 times according to formatting of meta-analysis data set

clark_data <- mutate(clark_data, Species=summary_stats$species, .after=10)
clark_data[which(clark_data$Species=="acantho"),]$Species <- "Acanthochromis polyacanthus"
clark_data[which(clark_data$Species=="ambon"),]$Species <- "Pomacentrus amboinensis"
clark_data[which(clark_data$Species=="chromis"),]$Species <- "Chromis atripectoralis"
clark_data[which(clark_data$Species=="humbug"),]$Species <- "Dascyllus aruanus"
clark_data[which(clark_data$Species=="lemon"),]$Species <- "Pomacentrus moluccensis"
clark_data[which(clark_data$Species=="whitedams"),]$Species <- "Dischistodus perspicillatus"

clark_data <- mutate(clark_data, 
                    ctrl.n=summary_stats$`n()_control`, 
                    ctrl.mean=summary_stats$`mean(activity)_control`, 
                    ctrl.sd=summary_stats$`sd(activity)_control`, 
                    oa.n=summary_stats$`n()_CO2`, 
                    oa.mean=summary_stats$`mean(activity)_CO2`, 
                    oa.sd=summary_stats$`sd(activity)_CO2`)
clark_data
```

#### Merge this combined summary statistics/meta-data with the larger meta-analysis data set in ocean_meta_data.csv
```{r}
meta_data <- read_csv("ocean_meta_data.csv")
clark_data$`Pub year IF` <- as.character(clark_data$`Pub year IF`) #Changing variable class to character to match meta-analysis data set 
clark_data$`2017 IF` <- as.character(clark_data$`2017 IF`) #Changing variable class to character to match meta-analysis data set 

meta_data <- full_join(meta_data, clark_data)
summary(meta_data)
```

```{r}
ggplot(meta_data, aes(x=oa.mean, y=oa.sd))+
  geom_point()
```

From the data summary and plot there are four clear outlier data points with very high means and standard deviations for both the control and treatment conditions. This will dramatically skew the data if left in, so these four points will be investigated. 

```{r}
meta_data %>% filter(meta_data$oa.mean > 2000)
```

It seems like the outlier data points could be due to all four rows investigating the larvae life stage, could be related to the species each study looked at or could be a measurement/unit error in both of the studies. Let's first investigate all of the larvae data.

```{r}
meta_data %>% filter(meta_data$`Life stage`=="Larvae")
```

There are plenty of other data points for the Larvae life stage which seem to fit with the rest of the data, so this doesn't seem to be the reason for the high numbers. Let's investigate the fish species. 

```{r}
meta_data %>% filter(meta_data$Species=="Atherina presbyter"|meta_data$Species=="Clupea harengus")
```

Again there is other data for both species which fits with the rest of the data. It seems like these two studies have extremely high values possibly due to a measurement difference or a unit error. It is clear that there has been an error in the standardisation of values for the meta-analysis. As such, it seems reasonable to exclude both of these studies from the meta-data. 

```{r}
meta_data <- meta_data %>% filter(meta_data$Title!="Effects of ocean acidification on the swimming ability, development and biochemical responses of sand smelt larvae" & meta_data$Title!="Growth performance and survival of larval Atlantic herring, under the combined effects of elevated temperatures and CO2")
```


#### Calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s escalc() function
```{r}
corr_stats <- cor.test(meta_data$ctrl.mean, meta_data$oa.mean, method="pearson")
meta_data$r_corr <- corr_stats$estimate
meta_data <- metafor::escalc(measure = "ROM", 
                                n1i = ctrl.n, n2i = oa.n,
                                m1i = ctrl.mean, m2i = oa.mean, 
                                sd1i = ctrl.sd, sd2i = oa.sd, 
                                ri = r_corr,
                                var.names=c("lnRR","V_lnRR"),
                                data = meta_data)
meta_data <- meta_data %>% mutate(residual = 1:n())
```

```{r}
meta_data %>% filter(meta_data$V_lnRR<=0.0001)
```
There are clearly some data points with extremely high variance compared to the vast majority of the studies. These will be excluded - data unreliable if variance is greater than 50, study must be severely under-powered. There are also some extremely small variances which will also be excluded. Expect variance to be greater than 0.0001 even for really high-powered studies. Looking at the studies, they all have relatively small sample sizes so would definitely expect biological variance to contribute to variance of the log response ratio. These extremely small variances are unrealistic and must be errors. 

Filter out high and low variances
```{r}
meta_data <- meta_data %>% filter(meta_data$V_lnRR<50)
meta_data <- meta_data %>% filter(meta_data$V_lnRR>0.0001)
```


#### Meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor’s rma.mv() function.
```{r}
model <- metafor::rma.mv(lnRR~1, V=V_lnRR, 
                         method="REML",
                         random=list(~1|Study,
                                     ~1|Behavioural.metric,
                                     ~1|residual),
                         dfs = "contain",
                         test="t",
                         data=meta_data)
model
```

```{r}
orchaRd::i2_ml(model, data = meta_data)
```


#### Written paragraph about the findings and what they mean. Support with a figure. Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals). Measures of heterogeneity in effect size estimates across studies (i.e., I2 and/or prediction intervals - see predict() function in metafor). 

Overall meta-analytic mean is -0.1532. This means that for every 1 increase in the control mean, the treatment mean increases by 0.8468 (1-0.1532). 
The 95% confidence interval is -0.3839 to 0.0776. This confidence interval includes 0 which is predicted under the null hypothesis, so there is no statistically significant difference between the treatment and control groups. The null hypothesis is retained. 

As seen in Figure below, although there is variability between studies there is a general linear trend between the means of the control and treatment groups. For example, if the mean of the control group is 500, then the mean of the treatment group is also likely to be around 500. This shows that there is no statistical difference between the control and treatment groups, so there is no indication that ocean acidification alters behaviours. 

There is a significant amount of heterogeneity among effects, with Q=73,608.9522, df=789, p=<0.0001. Proportion of variance among effects after removing sampling variation is 99.87955%. Differences between studies explains 11.86475% of the effect size variation, and differences between Behavioural metric explains 69.29247% of the effect size variation. 

```{r}
ggplot(meta_data, aes(x=ctrl.mean, y=oa.mean))+
  geom_point()+
  geom_smooth(method = "lm")+
  labs(x="Means of control groups", y="Means of treatment groups")
```

#### Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure.

K (790) is the number of effect sizes, the brackets give the number of studies (90). The size of the effect is scaled by the precision of each effect size value. 
```{r}
orchaRd::orchard_plot(model, group = "Study", data = meta_data, xlab = "Log response ratio (lnRR)", angle = 45)
```

It is clear that most of the studies are sitting around a log response ratio of 0, which indicates no difference between the control and treatment groups. Ocean acidification does not seem to have an effect on behaviour. 

#### Generate a funnel plot. Visually assess the possibility of publication bias.

```{r}
metafor::funnel(x = meta_data$lnRR, vi = meta_data$V_lnRR, 
                yaxis = "seinv", digits = 2, 
                level = c(0.1, 0.05, 0.01), 
                shade = c("white", "gray55", "gray 75"), las = 1, 
                xlab = "Log response ratio (lnRR)", ylab = "Precision (inverse standard error)",
                atransf=tanh, legend = TRUE)
```

From the funnel plot, it seems that there is some publication bias towards negative log response ratios. There is a gap on the bottom right hand side of the funnel plot where low precision, potentially under-powered studies would have found a positive log response ratio. This shows that there is likely a file-drawer problem, as authors probably did not publish these results due to opposing previous study results with a high amount of standard error. 

#### Generate a time-lag plot. Assess how effect sizes may or may not have changed through time.
```{r}
ggplot(meta_data, aes(y = lnRR, x = Year..online., size = 1/sqrt(V_lnRR))) + 
  geom_point(alpha = 0.30) + 
  geom_smooth(method = lm, col = "red", show.legend = FALSE) + 
  labs(x = "Online Publication Year", y = "Log Response Ratio (lnRR)", size = "Precision (1/SE)") + 
  theme_classic()
```


#### Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias.
```{r}
metafor::rma(yi = lnRR, vi = V_lnRR, method = "FE", data = meta_data)
```

There is a highly significant time-lag bias in this meta-data. 

#### Formal meta-regression model that includes inverse sampling variance (1/variance of log response ratio) to test for file-drawer biases
```{r}
# Including sampling variance as moderator
metareg_time <- rma.mv(lnRR ~ Year..online., V = 1/V_lnRR, 
                    random = list(~1|Study, 
                                  ~1|residual), 
                    test = "t", dfs = "contain", 
                    data = meta_data)
summary(metareg_time)

# How much variation does time when results were published explain in lnRR?
r2_time <- orchaRd::r2_ml(metareg_time) 
r2_time
```

Significant results indicate file-drawer biases. 

#### Written paragraph about the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

The log-response ratio is definitely converging on the value of 0 over time. In the first couple of years, there is a strong bias towards studies with negative log response ratios. Then in the middle there is a bias towards studies with positive log response ratios which drags the trend towards 0. By 2020, the studies are finding log response ratios close to 0. Also note that the early studies that found more extreme log response ratios typically have lower precision, whereas the higher precision studies are closer to a log response ratio of 0. 

#### Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et al? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?


